{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Parameters of Model\n",
    "- Model training goes through iterative process of the following.\n",
    "> 1. Estimate the Output\n",
    "> 2. Calculate loss between estimation and real answer\n",
    "> 3. Collect all devariative of the error for the parameters\n",
    "> 4. Optimize all parameters using *Gradient Descent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Before start training, check the device to train\n",
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "                else \"mps\" if torch.backends.mps.is_available()\n",
    "                else \"CPU\")\n",
    "\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 18),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "- `Hyperparameters` are the parameters that can control the model optimization process.\n",
    "- Different hyperparameter values can affect model training and convergence rate.\n",
    "\n",
    "<br>\n",
    "\n",
    "- When learning, the following hyperparameters must be defined.\n",
    "> - `epochs`: The number of times the Dataset is repeated.\n",
    "> - `batch_size`: The number of data samples spread through Neural Networks before the parameters are updated.\n",
    "> - `learning_rate`: The ratio to control the parameters of the model at each batch/epochs.  \n",
    "> Smaller values can lead to slower learning, and larger values can lead to unpredicted behavior during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Loop\n",
    "- After setting the hyperparameters, you can train and optimize your model with *Optimization Loop*.\n",
    "- Each *iteration* in the Optimization Loop is called an `Epoch`.\n",
    "\n",
    "<br>\n",
    "\n",
    "- An Epoch consists of the following parts.\n",
    "> - `Train Loop`: Iterates the training datasets and converges to the optimal parameters.\n",
    "> - `Test/Validation Loop`: Iterates the test datasets to check if the model performance is improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- When providing the training data, there is high probability that the untrained Neural Network will not provice the correct answer.\n",
    "- The *Loss Function* measures degree of dissimilarity between obtained result and actual value, and attempts to minimize it during training.\n",
    "- Calculates the *loss* by comparing prediction (calculated as input from given data samples) and real answer (*label*).\n",
    "\n",
    "<br>\n",
    "\n",
    "- General Loss Functions include...\n",
    "> - `nn.MSELoss`: MSE(Mean Square Error) for *Regression* task.\n",
    "> - `nn/NLLLoss`: NLL(Negative Log Likelihood) for for *Classification* task.\n",
    "> - `nn.CrossEntropyLoss`: The combination of `nn.LogSoftmax` and `nn.NLLLoss`.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Regularizes the *logits* by transferring output logit of model to `nn.CrossEntorpyLoss` and calculates predicted error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- *Optimization* is the process of controlling parameters of model at each step of training to reduce the model's error.\n",
    "- *Optimization Algorithm* dictates how to execute this process. IN this example, the algorithm is `SGD(Stochastic Gradient Descent)`.\n",
    "- All of logics for optimization are encapsulated in `optimizer` object.\n",
    "- In PyTorch, there are many types of optimizers such as `ADAM` and `RMSProp` that work better with certain types of models or data.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Register the parameters and learning rate of the model you want to train to initialize the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the training loop, optimization consists of 3 steps.\n",
    "> 1. Resets the gradient of model's parameters by calling `optimizer.zero_grad()`.  \n",
    "Since the gradient is added up, it has to be set to 0 to prevent duplicate calculations.\n",
    "> 2. Backpropagates the the *prediction loss* by calling `loss.backwards()`.  \n",
    "PyTorch saves the gradient of loss for each parameter.\n",
    "> 3. After calculating the gradient, control the parameters by using the gradient collected in the Backpropagation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Imprementation\n",
    "- Declared `train_loop`(executes the optimization codes repeatedly) and `test_loop`(evaluates the model's performance with test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Sets the model as Train mode. - It's important for the Batch Normalization and Dropout layers.\n",
    "    # It is not necessary in this example, but I have added it for best practice.\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Calculates with GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Calculation of prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d} / {size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Sets the model as Evaluation mode. - It's important for the Batch Normalization and Dropout layers.\n",
    "    # It is not necessary in this example, but I have added it for best practice.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Calculates with GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Average Loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the Loss Function and Optimizer and send them to `train_loop` and `test_loop`.\n",
    "- You can freely try increasing the number of `epochs` to observe the improvement of model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.895954  [   64 / 60000]\n",
      "loss: 2.859394  [ 6464 / 60000]\n",
      "loss: 2.811296  [12864 / 60000]\n",
      "loss: 2.779952  [19264 / 60000]\n",
      "loss: 2.726380  [25664 / 60000]\n",
      "loss: 2.648319  [32064 / 60000]\n",
      "loss: 2.634180  [38464 / 60000]\n",
      "loss: 2.533622  [44864 / 60000]\n",
      "loss: 2.502630  [51264 / 60000]\n",
      "loss: 2.420824  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 27.1%, Average Loss: 2.390184 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.406029  [   64 / 60000]\n",
      "loss: 2.383222  [ 6464 / 60000]\n",
      "loss: 2.261412  [12864 / 60000]\n",
      "loss: 2.278776  [19264 / 60000]\n",
      "loss: 2.210935  [25664 / 60000]\n",
      "loss: 2.082571  [32064 / 60000]\n",
      "loss: 2.157885  [38464 / 60000]\n",
      "loss: 2.019848  [44864 / 60000]\n",
      "loss: 2.041665  [51264 / 60000]\n",
      "loss: 1.944015  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Average Loss: 1.931624 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.973522  [   64 / 60000]\n",
      "loss: 1.938063  [ 6464 / 60000]\n",
      "loss: 1.785159  [12864 / 60000]\n",
      "loss: 1.829759  [19264 / 60000]\n",
      "loss: 1.714587  [25664 / 60000]\n",
      "loss: 1.651517  [32064 / 60000]\n",
      "loss: 1.678769  [38464 / 60000]\n",
      "loss: 1.555834  [44864 / 60000]\n",
      "loss: 1.593567  [51264 / 60000]\n",
      "loss: 1.472101  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Average Loss: 1.497184 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.568933  [   64 / 60000]\n",
      "loss: 1.536882  [ 6464 / 60000]\n",
      "loss: 1.371979  [12864 / 60000]\n",
      "loss: 1.446831  [19264 / 60000]\n",
      "loss: 1.331379  [25664 / 60000]\n",
      "loss: 1.330846  [32064 / 60000]\n",
      "loss: 1.341281  [38464 / 60000]\n",
      "loss: 1.253464  [44864 / 60000]\n",
      "loss: 1.303037  [51264 / 60000]\n",
      "loss: 1.199792  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Average Loss: 1.229954 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.306923  [   64 / 60000]\n",
      "loss: 1.291835  [ 6464 / 60000]\n",
      "loss: 1.121653  [12864 / 60000]\n",
      "loss: 1.227203  [19264 / 60000]\n",
      "loss: 1.115584  [25664 / 60000]\n",
      "loss: 1.140703  [32064 / 60000]\n",
      "loss: 1.156679  [38464 / 60000]\n",
      "loss: 1.082400  [44864 / 60000]\n",
      "loss: 1.136470  [51264 / 60000]\n",
      "loss: 1.050480  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Average Loss: 1.074134 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.143471  [   64 / 60000]\n",
      "loss: 1.147211  [ 6464 / 60000]\n",
      "loss: 0.963952  [12864 / 60000]\n",
      "loss: 1.094836  [19264 / 60000]\n",
      "loss: 0.987846  [25664 / 60000]\n",
      "loss: 1.016029  [32064 / 60000]\n",
      "loss: 1.046091  [38464 / 60000]\n",
      "loss: 0.977429  [44864 / 60000]\n",
      "loss: 1.028209  [51264 / 60000]\n",
      "loss: 0.958723  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Average Loss: 0.974398 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.030408  [   64 / 60000]\n",
      "loss: 1.054283  [ 6464 / 60000]\n",
      "loss: 0.856266  [12864 / 60000]\n",
      "loss: 1.007084  [19264 / 60000]\n",
      "loss: 0.906845  [25664 / 60000]\n",
      "loss: 0.927913  [32064 / 60000]\n",
      "loss: 0.973140  [38464 / 60000]\n",
      "loss: 0.910292  [44864 / 60000]\n",
      "loss: 0.952672  [51264 / 60000]\n",
      "loss: 0.896743  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Average Loss: 0.905833 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.946590  [   64 / 60000]\n",
      "loss: 0.988636  [ 6464 / 60000]\n",
      "loss: 0.778538  [12864 / 60000]\n",
      "loss: 0.944827  [19264 / 60000]\n",
      "loss: 0.851638  [25664 / 60000]\n",
      "loss: 0.862577  [32064 / 60000]\n",
      "loss: 0.920698  [38464 / 60000]\n",
      "loss: 0.865477  [44864 / 60000]\n",
      "loss: 0.897271  [51264 / 60000]\n",
      "loss: 0.851388  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Average Loss: 0.855745 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.881538  [   64 / 60000]\n",
      "loss: 0.938279  [ 6464 / 60000]\n",
      "loss: 0.719607  [12864 / 60000]\n",
      "loss: 0.898087  [19264 / 60000]\n",
      "loss: 0.811037  [25664 / 60000]\n",
      "loss: 0.812656  [32064 / 60000]\n",
      "loss: 0.880068  [38464 / 60000]\n",
      "loss: 0.834088  [44864 / 60000]\n",
      "loss: 0.855235  [51264 / 60000]\n",
      "loss: 0.816228  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Average Loss: 0.817261 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.829184  [   64 / 60000]\n",
      "loss: 0.897185  [ 6464 / 60000]\n",
      "loss: 0.673203  [12864 / 60000]\n",
      "loss: 0.861634  [19264 / 60000]\n",
      "loss: 0.779482  [25664 / 60000]\n",
      "loss: 0.773687  [32064 / 60000]\n",
      "loss: 0.846833  [38464 / 60000]\n",
      "loss: 0.810698  [44864 / 60000]\n",
      "loss: 0.822384  [51264 / 60000]\n",
      "loss: 0.787574  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Average Loss: 0.786367 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.785570  [   64 / 60000]\n",
      "loss: 0.862179  [ 6464 / 60000]\n",
      "loss: 0.635339  [12864 / 60000]\n",
      "loss: 0.832534  [19264 / 60000]\n",
      "loss: 0.753919  [25664 / 60000]\n",
      "loss: 0.742650  [32064 / 60000]\n",
      "loss: 0.818464  [38464 / 60000]\n",
      "loss: 0.791786  [44864 / 60000]\n",
      "loss: 0.795929  [51264 / 60000]\n",
      "loss: 0.763359  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Average Loss: 0.760559 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.748383  [   64 / 60000]\n",
      "loss: 0.831200  [ 6464 / 60000]\n",
      "loss: 0.603487  [12864 / 60000]\n",
      "loss: 0.808582  [19264 / 60000]\n",
      "loss: 0.732225  [25664 / 60000]\n",
      "loss: 0.717252  [32064 / 60000]\n",
      "loss: 0.793161  [38464 / 60000]\n",
      "loss: 0.775770  [44864 / 60000]\n",
      "loss: 0.773774  [51264 / 60000]\n",
      "loss: 0.742398  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Average Loss: 0.738232 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.715994  [   64 / 60000]\n",
      "loss: 0.803239  [ 6464 / 60000]\n",
      "loss: 0.575980  [12864 / 60000]\n",
      "loss: 0.788285  [19264 / 60000]\n",
      "loss: 0.713631  [25664 / 60000]\n",
      "loss: 0.696108  [32064 / 60000]\n",
      "loss: 0.770108  [38464 / 60000]\n",
      "loss: 0.761339  [44864 / 60000]\n",
      "loss: 0.754600  [51264 / 60000]\n",
      "loss: 0.723667  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Average Loss: 0.718505 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.687444  [   64 / 60000]\n",
      "loss: 0.777934  [ 6464 / 60000]\n",
      "loss: 0.552165  [12864 / 60000]\n",
      "loss: 0.770493  [19264 / 60000]\n",
      "loss: 0.697286  [25664 / 60000]\n",
      "loss: 0.678388  [32064 / 60000]\n",
      "loss: 0.748636  [38464 / 60000]\n",
      "loss: 0.748092  [44864 / 60000]\n",
      "loss: 0.737635  [51264 / 60000]\n",
      "loss: 0.706710  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Average Loss: 0.700735 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.662158  [   64 / 60000]\n",
      "loss: 0.754717  [ 6464 / 60000]\n",
      "loss: 0.531231  [12864 / 60000]\n",
      "loss: 0.754484  [19264 / 60000]\n",
      "loss: 0.683035  [25664 / 60000]\n",
      "loss: 0.663148  [32064 / 60000]\n",
      "loss: 0.728373  [38464 / 60000]\n",
      "loss: 0.735871  [44864 / 60000]\n",
      "loss: 0.722757  [51264 / 60000]\n",
      "loss: 0.691293  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Average Loss: 0.684552 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.639378  [   64 / 60000]\n",
      "loss: 0.733519  [ 6464 / 60000]\n",
      "loss: 0.512565  [12864 / 60000]\n",
      "loss: 0.739760  [19264 / 60000]\n",
      "loss: 0.670468  [25664 / 60000]\n",
      "loss: 0.649940  [32064 / 60000]\n",
      "loss: 0.709528  [38464 / 60000]\n",
      "loss: 0.724702  [44864 / 60000]\n",
      "loss: 0.709574  [51264 / 60000]\n",
      "loss: 0.677183  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Average Loss: 0.669736 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.618753  [   64 / 60000]\n",
      "loss: 0.714124  [ 6464 / 60000]\n",
      "loss: 0.495735  [12864 / 60000]\n",
      "loss: 0.726301  [19264 / 60000]\n",
      "loss: 0.659162  [25664 / 60000]\n",
      "loss: 0.638339  [32064 / 60000]\n",
      "loss: 0.691814  [38464 / 60000]\n",
      "loss: 0.714669  [44864 / 60000]\n",
      "loss: 0.698047  [51264 / 60000]\n",
      "loss: 0.664095  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Average Loss: 0.656138 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.600191  [   64 / 60000]\n",
      "loss: 0.696319  [ 6464 / 60000]\n",
      "loss: 0.480741  [12864 / 60000]\n",
      "loss: 0.713828  [19264 / 60000]\n",
      "loss: 0.649145  [25664 / 60000]\n",
      "loss: 0.628190  [32064 / 60000]\n",
      "loss: 0.675365  [38464 / 60000]\n",
      "loss: 0.705779  [44864 / 60000]\n",
      "loss: 0.687934  [51264 / 60000]\n",
      "loss: 0.651816  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Average Loss: 0.643632 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.583388  [   64 / 60000]\n",
      "loss: 0.680051  [ 6464 / 60000]\n",
      "loss: 0.467160  [12864 / 60000]\n",
      "loss: 0.702210  [19264 / 60000]\n",
      "loss: 0.640158  [25664 / 60000]\n",
      "loss: 0.619269  [32064 / 60000]\n",
      "loss: 0.659997  [38464 / 60000]\n",
      "loss: 0.698001  [44864 / 60000]\n",
      "loss: 0.679259  [51264 / 60000]\n",
      "loss: 0.640339  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Average Loss: 0.632124 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.568122  [   64 / 60000]\n",
      "loss: 0.665123  [ 6464 / 60000]\n",
      "loss: 0.454842  [12864 / 60000]\n",
      "loss: 0.691384  [19264 / 60000]\n",
      "loss: 0.631998  [25664 / 60000]\n",
      "loss: 0.611291  [32064 / 60000]\n",
      "loss: 0.645620  [38464 / 60000]\n",
      "loss: 0.691268  [44864 / 60000]\n",
      "loss: 0.671823  [51264 / 60000]\n",
      "loss: 0.629456  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Average Loss: 0.621526 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.554222  [   64 / 60000]\n",
      "loss: 0.651412  [ 6464 / 60000]\n",
      "loss: 0.443558  [12864 / 60000]\n",
      "loss: 0.681198  [19264 / 60000]\n",
      "loss: 0.624478  [25664 / 60000]\n",
      "loss: 0.604104  [32064 / 60000]\n",
      "loss: 0.632177  [38464 / 60000]\n",
      "loss: 0.685494  [44864 / 60000]\n",
      "loss: 0.665268  [51264 / 60000]\n",
      "loss: 0.619237  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Average Loss: 0.611777 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.541553  [   64 / 60000]\n",
      "loss: 0.638797  [ 6464 / 60000]\n",
      "loss: 0.433240  [12864 / 60000]\n",
      "loss: 0.671619  [19264 / 60000]\n",
      "loss: 0.617421  [25664 / 60000]\n",
      "loss: 0.597577  [32064 / 60000]\n",
      "loss: 0.619691  [38464 / 60000]\n",
      "loss: 0.680636  [44864 / 60000]\n",
      "loss: 0.659697  [51264 / 60000]\n",
      "loss: 0.609533  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Average Loss: 0.602791 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.529921  [   64 / 60000]\n",
      "loss: 0.627235  [ 6464 / 60000]\n",
      "loss: 0.423782  [12864 / 60000]\n",
      "loss: 0.662618  [19264 / 60000]\n",
      "loss: 0.610685  [25664 / 60000]\n",
      "loss: 0.591539  [32064 / 60000]\n",
      "loss: 0.608045  [38464 / 60000]\n",
      "loss: 0.676539  [44864 / 60000]\n",
      "loss: 0.654876  [51264 / 60000]\n",
      "loss: 0.600289  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Average Loss: 0.594487 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.519141  [   64 / 60000]\n",
      "loss: 0.616621  [ 6464 / 60000]\n",
      "loss: 0.415060  [12864 / 60000]\n",
      "loss: 0.654109  [19264 / 60000]\n",
      "loss: 0.604146  [25664 / 60000]\n",
      "loss: 0.585872  [32064 / 60000]\n",
      "loss: 0.597225  [38464 / 60000]\n",
      "loss: 0.673293  [44864 / 60000]\n",
      "loss: 0.650710  [51264 / 60000]\n",
      "loss: 0.591388  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Average Loss: 0.586803 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.509145  [   64 / 60000]\n",
      "loss: 0.606836  [ 6464 / 60000]\n",
      "loss: 0.406987  [12864 / 60000]\n",
      "loss: 0.646023  [19264 / 60000]\n",
      "loss: 0.597741  [25664 / 60000]\n",
      "loss: 0.580487  [32064 / 60000]\n",
      "loss: 0.587169  [38464 / 60000]\n",
      "loss: 0.670717  [44864 / 60000]\n",
      "loss: 0.647104  [51264 / 60000]\n",
      "loss: 0.582902  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Average Loss: 0.579674 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.499830  [   64 / 60000]\n",
      "loss: 0.597735  [ 6464 / 60000]\n",
      "loss: 0.399559  [12864 / 60000]\n",
      "loss: 0.638339  [19264 / 60000]\n",
      "loss: 0.591380  [25664 / 60000]\n",
      "loss: 0.575307  [32064 / 60000]\n",
      "loss: 0.577850  [38464 / 60000]\n",
      "loss: 0.668811  [44864 / 60000]\n",
      "loss: 0.643933  [51264 / 60000]\n",
      "loss: 0.574623  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Average Loss: 0.573055 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.491099  [   64 / 60000]\n",
      "loss: 0.589300  [ 6464 / 60000]\n",
      "loss: 0.392709  [12864 / 60000]\n",
      "loss: 0.631000  [19264 / 60000]\n",
      "loss: 0.585063  [25664 / 60000]\n",
      "loss: 0.570305  [32064 / 60000]\n",
      "loss: 0.569191  [38464 / 60000]\n",
      "loss: 0.667467  [44864 / 60000]\n",
      "loss: 0.641181  [51264 / 60000]\n",
      "loss: 0.566538  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Average Loss: 0.566900 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.482888  [   64 / 60000]\n",
      "loss: 0.581548  [ 6464 / 60000]\n",
      "loss: 0.386359  [12864 / 60000]\n",
      "loss: 0.623969  [19264 / 60000]\n",
      "loss: 0.578794  [25664 / 60000]\n",
      "loss: 0.565395  [32064 / 60000]\n",
      "loss: 0.561154  [38464 / 60000]\n",
      "loss: 0.666621  [44864 / 60000]\n",
      "loss: 0.638806  [51264 / 60000]\n",
      "loss: 0.558671  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Average Loss: 0.561167 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.475123  [   64 / 60000]\n",
      "loss: 0.574357  [ 6464 / 60000]\n",
      "loss: 0.380405  [12864 / 60000]\n",
      "loss: 0.617267  [19264 / 60000]\n",
      "loss: 0.572540  [25664 / 60000]\n",
      "loss: 0.560526  [32064 / 60000]\n",
      "loss: 0.553696  [38464 / 60000]\n",
      "loss: 0.666178  [44864 / 60000]\n",
      "loss: 0.636799  [51264 / 60000]\n",
      "loss: 0.551019  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Average Loss: 0.555819 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.467730  [   64 / 60000]\n",
      "loss: 0.567691  [ 6464 / 60000]\n",
      "loss: 0.374781  [12864 / 60000]\n",
      "loss: 0.610883  [19264 / 60000]\n",
      "loss: 0.566412  [25664 / 60000]\n",
      "loss: 0.555709  [32064 / 60000]\n",
      "loss: 0.546720  [38464 / 60000]\n",
      "loss: 0.666052  [44864 / 60000]\n",
      "loss: 0.634960  [51264 / 60000]\n",
      "loss: 0.543594  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Average Loss: 0.550817 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.460745  [   64 / 60000]\n",
      "loss: 0.561530  [ 6464 / 60000]\n",
      "loss: 0.369482  [12864 / 60000]\n",
      "loss: 0.604742  [19264 / 60000]\n",
      "loss: 0.560184  [25664 / 60000]\n",
      "loss: 0.550934  [32064 / 60000]\n",
      "loss: 0.540170  [38464 / 60000]\n",
      "loss: 0.666183  [44864 / 60000]\n",
      "loss: 0.633174  [51264 / 60000]\n",
      "loss: 0.536359  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Average Loss: 0.546130 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.454077  [   64 / 60000]\n",
      "loss: 0.555768  [ 6464 / 60000]\n",
      "loss: 0.364561  [12864 / 60000]\n",
      "loss: 0.598835  [19264 / 60000]\n",
      "loss: 0.554056  [25664 / 60000]\n",
      "loss: 0.546192  [32064 / 60000]\n",
      "loss: 0.534068  [38464 / 60000]\n",
      "loss: 0.666549  [44864 / 60000]\n",
      "loss: 0.631492  [51264 / 60000]\n",
      "loss: 0.529336  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Average Loss: 0.541737 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.447743  [   64 / 60000]\n",
      "loss: 0.550407  [ 6464 / 60000]\n",
      "loss: 0.359953  [12864 / 60000]\n",
      "loss: 0.593143  [19264 / 60000]\n",
      "loss: 0.548018  [25664 / 60000]\n",
      "loss: 0.541506  [32064 / 60000]\n",
      "loss: 0.528367  [38464 / 60000]\n",
      "loss: 0.667005  [44864 / 60000]\n",
      "loss: 0.629846  [51264 / 60000]\n",
      "loss: 0.522591  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Average Loss: 0.537609 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.441598  [   64 / 60000]\n",
      "loss: 0.545450  [ 6464 / 60000]\n",
      "loss: 0.355596  [12864 / 60000]\n",
      "loss: 0.587689  [19264 / 60000]\n",
      "loss: 0.542095  [25664 / 60000]\n",
      "loss: 0.536908  [32064 / 60000]\n",
      "loss: 0.522995  [38464 / 60000]\n",
      "loss: 0.667438  [44864 / 60000]\n",
      "loss: 0.628212  [51264 / 60000]\n",
      "loss: 0.516066  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Average Loss: 0.533726 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.435709  [   64 / 60000]\n",
      "loss: 0.540830  [ 6464 / 60000]\n",
      "loss: 0.351513  [12864 / 60000]\n",
      "loss: 0.582435  [19264 / 60000]\n",
      "loss: 0.536272  [25664 / 60000]\n",
      "loss: 0.532366  [32064 / 60000]\n",
      "loss: 0.517942  [38464 / 60000]\n",
      "loss: 0.667901  [44864 / 60000]\n",
      "loss: 0.626616  [51264 / 60000]\n",
      "loss: 0.509758  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Average Loss: 0.530070 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.430036  [   64 / 60000]\n",
      "loss: 0.536471  [ 6464 / 60000]\n",
      "loss: 0.347668  [12864 / 60000]\n",
      "loss: 0.577398  [19264 / 60000]\n",
      "loss: 0.530645  [25664 / 60000]\n",
      "loss: 0.527904  [32064 / 60000]\n",
      "loss: 0.513263  [38464 / 60000]\n",
      "loss: 0.668345  [44864 / 60000]\n",
      "loss: 0.625012  [51264 / 60000]\n",
      "loss: 0.503710  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Average Loss: 0.526620 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.424545  [   64 / 60000]\n",
      "loss: 0.532376  [ 6464 / 60000]\n",
      "loss: 0.344036  [12864 / 60000]\n",
      "loss: 0.572609  [19264 / 60000]\n",
      "loss: 0.525164  [25664 / 60000]\n",
      "loss: 0.523500  [32064 / 60000]\n",
      "loss: 0.508872  [38464 / 60000]\n",
      "loss: 0.668736  [44864 / 60000]\n",
      "loss: 0.623449  [51264 / 60000]\n",
      "loss: 0.497952  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Average Loss: 0.523357 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.419266  [   64 / 60000]\n",
      "loss: 0.528574  [ 6464 / 60000]\n",
      "loss: 0.340589  [12864 / 60000]\n",
      "loss: 0.568010  [19264 / 60000]\n",
      "loss: 0.519799  [25664 / 60000]\n",
      "loss: 0.519155  [32064 / 60000]\n",
      "loss: 0.504748  [38464 / 60000]\n",
      "loss: 0.669032  [44864 / 60000]\n",
      "loss: 0.621882  [51264 / 60000]\n",
      "loss: 0.492431  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Average Loss: 0.520264 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.414132  [   64 / 60000]\n",
      "loss: 0.525011  [ 6464 / 60000]\n",
      "loss: 0.337332  [12864 / 60000]\n",
      "loss: 0.563566  [19264 / 60000]\n",
      "loss: 0.514565  [25664 / 60000]\n",
      "loss: 0.514936  [32064 / 60000]\n",
      "loss: 0.500826  [38464 / 60000]\n",
      "loss: 0.669172  [44864 / 60000]\n",
      "loss: 0.620323  [51264 / 60000]\n",
      "loss: 0.487156  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Average Loss: 0.517327 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.409173  [   64 / 60000]\n",
      "loss: 0.521637  [ 6464 / 60000]\n",
      "loss: 0.334244  [12864 / 60000]\n",
      "loss: 0.559269  [19264 / 60000]\n",
      "loss: 0.509508  [25664 / 60000]\n",
      "loss: 0.510888  [32064 / 60000]\n",
      "loss: 0.497177  [38464 / 60000]\n",
      "loss: 0.669228  [44864 / 60000]\n",
      "loss: 0.618667  [51264 / 60000]\n",
      "loss: 0.482102  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Average Loss: 0.514530 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.404374  [   64 / 60000]\n",
      "loss: 0.518436  [ 6464 / 60000]\n",
      "loss: 0.331336  [12864 / 60000]\n",
      "loss: 0.555125  [19264 / 60000]\n",
      "loss: 0.504611  [25664 / 60000]\n",
      "loss: 0.506938  [32064 / 60000]\n",
      "loss: 0.493720  [38464 / 60000]\n",
      "loss: 0.669070  [44864 / 60000]\n",
      "loss: 0.617000  [51264 / 60000]\n",
      "loss: 0.477268  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Average Loss: 0.511863 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.399693  [   64 / 60000]\n",
      "loss: 0.515413  [ 6464 / 60000]\n",
      "loss: 0.328545  [12864 / 60000]\n",
      "loss: 0.551116  [19264 / 60000]\n",
      "loss: 0.499886  [25664 / 60000]\n",
      "loss: 0.503108  [32064 / 60000]\n",
      "loss: 0.490430  [38464 / 60000]\n",
      "loss: 0.668780  [44864 / 60000]\n",
      "loss: 0.615344  [51264 / 60000]\n",
      "loss: 0.472663  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Average Loss: 0.509317 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.395163  [   64 / 60000]\n",
      "loss: 0.512571  [ 6464 / 60000]\n",
      "loss: 0.325878  [12864 / 60000]\n",
      "loss: 0.547277  [19264 / 60000]\n",
      "loss: 0.495293  [25664 / 60000]\n",
      "loss: 0.499435  [32064 / 60000]\n",
      "loss: 0.487285  [38464 / 60000]\n",
      "loss: 0.668386  [44864 / 60000]\n",
      "loss: 0.613653  [51264 / 60000]\n",
      "loss: 0.468284  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Average Loss: 0.506886 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.390758  [   64 / 60000]\n",
      "loss: 0.509862  [ 6464 / 60000]\n",
      "loss: 0.323319  [12864 / 60000]\n",
      "loss: 0.543624  [19264 / 60000]\n",
      "loss: 0.490857  [25664 / 60000]\n",
      "loss: 0.495866  [32064 / 60000]\n",
      "loss: 0.484268  [38464 / 60000]\n",
      "loss: 0.667862  [44864 / 60000]\n",
      "loss: 0.611986  [51264 / 60000]\n",
      "loss: 0.464140  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Average Loss: 0.504565 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.386461  [   64 / 60000]\n",
      "loss: 0.507321  [ 6464 / 60000]\n",
      "loss: 0.320883  [12864 / 60000]\n",
      "loss: 0.540135  [19264 / 60000]\n",
      "loss: 0.486543  [25664 / 60000]\n",
      "loss: 0.492409  [32064 / 60000]\n",
      "loss: 0.481377  [38464 / 60000]\n",
      "loss: 0.667282  [44864 / 60000]\n",
      "loss: 0.610342  [51264 / 60000]\n",
      "loss: 0.460280  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Average Loss: 0.502340 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.382312  [   64 / 60000]\n",
      "loss: 0.504919  [ 6464 / 60000]\n",
      "loss: 0.318547  [12864 / 60000]\n",
      "loss: 0.536762  [19264 / 60000]\n",
      "loss: 0.482380  [25664 / 60000]\n",
      "loss: 0.489082  [32064 / 60000]\n",
      "loss: 0.478629  [38464 / 60000]\n",
      "loss: 0.666615  [44864 / 60000]\n",
      "loss: 0.608687  [51264 / 60000]\n",
      "loss: 0.456616  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Average Loss: 0.500207 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.378258  [   64 / 60000]\n",
      "loss: 0.502638  [ 6464 / 60000]\n",
      "loss: 0.316288  [12864 / 60000]\n",
      "loss: 0.533511  [19264 / 60000]\n",
      "loss: 0.478358  [25664 / 60000]\n",
      "loss: 0.485864  [32064 / 60000]\n",
      "loss: 0.475977  [38464 / 60000]\n",
      "loss: 0.665856  [44864 / 60000]\n",
      "loss: 0.607058  [51264 / 60000]\n",
      "loss: 0.453138  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Average Loss: 0.498155 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.374326  [   64 / 60000]\n",
      "loss: 0.500437  [ 6464 / 60000]\n",
      "loss: 0.314101  [12864 / 60000]\n",
      "loss: 0.530423  [19264 / 60000]\n",
      "loss: 0.474465  [25664 / 60000]\n",
      "loss: 0.482728  [32064 / 60000]\n",
      "loss: 0.473453  [38464 / 60000]\n",
      "loss: 0.664958  [44864 / 60000]\n",
      "loss: 0.605443  [51264 / 60000]\n",
      "loss: 0.449845  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Average Loss: 0.496187 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.370480  [   64 / 60000]\n",
      "loss: 0.498303  [ 6464 / 60000]\n",
      "loss: 0.311980  [12864 / 60000]\n",
      "loss: 0.527466  [19264 / 60000]\n",
      "loss: 0.470711  [25664 / 60000]\n",
      "loss: 0.479713  [32064 / 60000]\n",
      "loss: 0.471031  [38464 / 60000]\n",
      "loss: 0.663938  [44864 / 60000]\n",
      "loss: 0.603842  [51264 / 60000]\n",
      "loss: 0.446734  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Average Loss: 0.494289 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.366747  [   64 / 60000]\n",
      "loss: 0.496258  [ 6464 / 60000]\n",
      "loss: 0.309950  [12864 / 60000]\n",
      "loss: 0.524613  [19264 / 60000]\n",
      "loss: 0.467088  [25664 / 60000]\n",
      "loss: 0.476830  [32064 / 60000]\n",
      "loss: 0.468696  [38464 / 60000]\n",
      "loss: 0.662831  [44864 / 60000]\n",
      "loss: 0.602274  [51264 / 60000]\n",
      "loss: 0.443794  [57664 / 60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Average Loss: 0.492460 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
