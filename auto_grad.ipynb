{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation with `torch.autograd`\n",
    "- *Backpropagation* is the most frequently used algorithm for training Neural Network.\n",
    "- Parameters(model weights) are adjusted by the gradient of the Loss Function for a given parameter.\n",
    "- To calculate those gradients, PyTorch has a bulit-in automatic differentiation engine called `torch.autograd`.  \n",
    "And it supports automatic calculation of gradients for all types of calculation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input Tensor(x) and Expected Output(y)\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "# Parameters\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Single-layered Neural Network and Loss Function\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this Neural Network, w and b are the parameters that should be optimized.\n",
    "- Therefore, you must be able to calculate the gradients of Loss Function for those parameters with `requires_grad` attiribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Function for z = <AddBackward0 object at 0x000002C007D34F10>\n",
      "Gradient Function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002C0084E64D0>\n"
     ]
    }
   ],
   "source": [
    "# Print gradient fuctions for Neural Network and Loss\n",
    "print(f\"Gradient Function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient Function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradient\n",
    "- To optimize the weights of parameters in a Neural Network, you must calculate the derivative of Loss Function with respect to the parameters.\n",
    "- For this, you must get the value from `w.grad` and `b.grad` after calling `loss_backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0034, 0.3219, 0.3229],\n",
      "        [0.0034, 0.3219, 0.3229],\n",
      "        [0.0034, 0.3219, 0.3229],\n",
      "        [0.0034, 0.3219, 0.3229],\n",
      "        [0.0034, 0.3219, 0.3229]])\n",
      "tensor([0.0034, 0.3219, 0.3229])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Stop Tracking the Gradient Change\n",
    "- Basically, all of the Tensors with `require_grads=True` track the calcuation history and support the calcluation of gradients.  \n",
    "But if you only need Net Propagation, this tracking or support may not be necessary.\n",
    "- You can stop tracking the calculation by surrounding the calculation codes with `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `detach()` method to Tensor also do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's why you should stop tracking gradient change:\n",
    "- Displays some of the parameters in a Neural Network as *frozen parameters*.\n",
    "- Since the calculation of Tensors that are not tracking gradient change is more efficient,  \n",
    "the calcuation speed will be improved when only the Net Propagation is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information about Calculation Graph\n",
    "- Conceptually, `autograd` keeps records of data(Tensor) and all operations executed(even if the operation result is a new Tensor)  \n",
    "in a *Directed Acyclic Graph(DAG)* composed of `Function` objects.\n",
    "- The leave of DAG is Input Tensor, and the root is Output.\n",
    "- Tracking this graph from the root to leaves makes you to calculate gradient automatically according to the *Chain Rule*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- In the Net Propagation step, `autograd` performs the following tasks at the same time.\n",
    "> - Calculates the Output Tensor by performing the qequested operation.\n",
    "> - Maintain the gradient function of DAG.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- The Backpropagation starts when `.backward()` method is called from the root of DAG.\n",
    "- At this time, `autograd` does...\n",
    "> - Calculate gradients from each `grad_fn`,\n",
    "> - Accumulates the result of operation to `.grad` attiributes of each Tensor,\n",
    "> - And Propagates to the all leaf Tensors using Chain Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Reading: Change of Tensor Gradient and Jacobian Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Call:\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "Second Call:\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "Call after zeroing gradients:\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.eye(4, 5, requires_grad=True)\n",
    "output = (input + 1).pow(2).t()\n",
    "\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "\n",
    "print(f\"First Call:\\n{input.grad}\")\n",
    "\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "\n",
    "print(f\"Second Call:\\n{input.grad}\")\n",
    "\n",
    "input.grad.zero_()\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "\n",
    "print(f\"Call after zeroing gradients:\\n{input.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
